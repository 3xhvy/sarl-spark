{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07dbd968",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 16:56:04,605 - INFO - Spark Version: 3.4.0\n",
      "2025-06-03 16:56:04,997 - INFO - Spark Driver Memory: 24g\n",
      "2025-06-03 16:56:04,998 - INFO - Spark Executor Memory: 24g\n",
      "2025-06-03 16:56:04,999 - INFO - Spark Executor Cores: 6\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.fpm import FPGrowth\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StructType, StructField, LongType, StringType, ArrayType\n",
    "from graphframes import GraphFrame\n",
    "\n",
    "# --- Initialize Logger ---\n",
    "# Set up basic logging (adjust level and format as needed)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- Spark Configuration (assuming you have this configured from previous steps) ---\n",
    "conf = SparkConf() \\\n",
    "    .setAppName(\"SARL_Spark_LPA_Partitioning\") \\\n",
    "    .setMaster(\"local[*]\") \\\n",
    "    .set(\"spark.driver.memory\", \"24g\") \\\n",
    "    .set(\"spark.executor.memory\", \"24g\") \\\n",
    "    .set(\"spark.executor.instances\", \"6\") \\\n",
    "    .set(\"spark.executor.cores\", \"6\") \\\n",
    "    .set(\"spark.memory.offHeap.enabled\", \"true\") \\\n",
    "    .set(\"spark.memory.offHeap.size\", \"3g\") \\\n",
    "    .set(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .set(\"spark.executor.extraJavaOptions\", \"-Xss4m\") \\\n",
    "    .set(\"spark.driver.extraJavaOptions\", \"-Xss4m\") \\\n",
    "    .set(\"spark.executor.memoryOverhead\", \"8g\") \\\n",
    "    .set(\"spark.driver.memoryOverhead\", \"8g\") \\\n",
    "    .set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .set(\"spark.kryoserializer.buffer.max\", \"512m\") \\\n",
    "    .set(\"spark.memory.fraction\", \"0.6\") \\\n",
    "    .set(\"spark.memory.storageFraction\", \"0.5\") \\\n",
    "    .set(\"spark.jars.packages\", \"graphframes:graphframes:0.8.2-spark3.2-s_2.12\")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "sc = spark.sparkContext # Get SparkContext for log level setting\n",
    "sc.setLogLevel(\"INFO\") # Set Spark's own logging level\n",
    "logger.info(f\"Spark Version: {spark.version}\")\n",
    "logger.info(f\"Spark Driver Memory: {spark.conf.get('spark.driver.memory')}\")\n",
    "logger.info(f\"Spark Executor Memory: {spark.conf.get('spark.executor.memory')}\")\n",
    "logger.info(f\"Spark Executor Cores: {spark.conf.get('spark.executor.cores')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fac76b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 16:58:24,128 - INFO - Total transactions loaded for baseline: 2302\n",
      "2025-06-03 16:58:24,129 - INFO - Baseline Parameters: min_support_percentage=0.03, min_confidence=0.7\n",
      "2025-06-03 16:58:24,129 - INFO - Running FPGrowth (Baseline) to find Frequent Itemsets...\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 38002)\n",
      "2025-06-03 16:58:59,039 - INFO - Error while receiving.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "Traceback (most recent call last):\n",
      "2025-06-03 16:58:59,041 - INFO - Closing down clientserver connection\n",
      "  File \"/opt/conda/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/conda/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/usr/local/spark/python/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n",
      "2025-06-03 16:58:59,041 - ERROR - Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "2025-06-03 16:58:59,041 - INFO - Closing down clientserver connection\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o247.count",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 49\u001b[0m\n\u001b[1;32m     47\u001b[0m frequent_itemsets_baseline \u001b[38;5;241m=\u001b[39m model_fp\u001b[38;5;241m.\u001b[39mfreqItemsets\n\u001b[1;32m     48\u001b[0m frequent_itemsets_baseline\u001b[38;5;241m.\u001b[39mcache()\n\u001b[0;32m---> 49\u001b[0m num_frequent_itemsets_baseline \u001b[38;5;241m=\u001b[39m \u001b[43mfrequent_itemsets_baseline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m time_fp_itemsets \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time_fp\n\u001b[1;32m     51\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFPGrowth Baseline: Found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_frequent_itemsets_baseline\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m frequent itemsets in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime_fp_itemsets\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:1193\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m   1171\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   1172\u001b[0m \n\u001b[1;32m   1173\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m                 \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m answer[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o247.count"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 16:59:00,031 - INFO - Error while receiving.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "2025-06-03 16:59:00,033 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,034 - ERROR - Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "2025-06-03 16:59:00,035 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,036 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,036 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,036 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,037 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,037 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,037 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,038 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,038 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,038 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,039 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,039 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,039 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,039 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,039 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,040 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,040 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,040 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,040 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,041 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,041 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,041 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,041 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,042 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,042 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,042 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,042 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,043 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,043 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,043 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,044 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,044 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,044 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,044 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,045 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,045 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,045 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,046 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,046 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,046 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,047 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,047 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,047 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,048 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,048 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,048 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,049 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,049 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,049 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,050 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,050 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,050 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,051 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,051 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,051 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,052 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,052 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,052 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,053 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,053 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,053 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,054 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,054 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,054 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,054 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,055 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,055 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,056 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,056 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,056 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,056 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,057 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,057 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,058 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,058 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,058 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,058 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,059 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,059 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,060 - INFO - Closing down clientserver connection\n",
      "2025-06-03 16:59:00,060 - INFO - Closing down clientserver connection\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Load Transactional Data (Consistent with SARL-Spark pipeline) ---\n",
    "transaction_file_path = 'sarl-spark/data/processed/transactional_data_zscore.csv'\n",
    "\n",
    "raw_lines_rdd = sc.textFile(transaction_file_path)\n",
    "header = raw_lines_rdd.first()\n",
    "\n",
    "# Define a helper function to process each line, ensuring 're' is available in worker scope\n",
    "def _process_transaction_line(line):\n",
    "    # Import 're' inside the function to ensure it's available on Spark workers\n",
    "    import re\n",
    "    items = re.findall(r'[+-]?[a-zA-Z0-9_]+', line)\n",
    "    # Filter out 'tumor' and 'normal' if they are labels\n",
    "    filtered_items = [item for item in items if item.lower() not in ['tumor', 'normal']]\n",
    "    # Ensure items within a transaction are unique by converting to set and back to list\n",
    "    unique_items = list(set(filtered_items))\n",
    "    return unique_items\n",
    "\n",
    "transactions_rdd = raw_lines_rdd.filter(lambda line: line != header) \\\n",
    "                                 .map(_process_transaction_line) \\\n",
    "                                 .filter(lambda transaction: len(transaction) > 0)\n",
    "\n",
    "transactions_df = spark.createDataFrame(transactions_rdd, ArrayType(StringType()))\n",
    "transactions_df = transactions_df.withColumnRenamed(\"value\", \"items\")\n",
    "transactions_df.cache()\n",
    "total_transactions = transactions_df.count() # Needed for Lift calculation if you extend this\n",
    "logger.info(f\"Total transactions loaded for baseline: {total_transactions}\")\n",
    "\n",
    "\n",
    "# --- SARL Heuristic Parameters (Consistent with SARL-Spark pipeline) ---\n",
    "MIN_SUPPORT_PERCENTAGE = 0.03 # Example: 10% minsup\n",
    "MIN_CONFIDENCE = 0.7 # Example: 70% minconf\n",
    "\n",
    "logger.info(f\"Baseline Parameters: min_support_percentage={MIN_SUPPORT_PERCENTAGE}, min_confidence={MIN_CONFIDENCE}\")\n",
    "\n",
    "# --- 1. Run FPGrowth to find Frequent Itemsets (Baseline) ---\n",
    "logger.info(\"Running FPGrowth (Baseline) to find Frequent Itemsets...\")\n",
    "start_time_fp = time.time()\n",
    "\n",
    "# Configure FPGrowth - numPartitions here controls internal parallelism for the whole dataset\n",
    "# FIX: Cast spark.conf.get() result to int before passing to numPartitions\n",
    "fpGrowth = FPGrowth(itemsCol=\"items\", minSupport=MIN_SUPPORT_PERCENTAGE, numPartitions=int(spark.conf.get(\"spark.sql.shuffle.partitions\"))) # Use shuffle partitions as a heuristic\n",
    "\n",
    "# Fit the model to the entire transactions_df\n",
    "model_fp = fpGrowth.fit(transactions_df)\n",
    "\n",
    "# Get frequent itemsets\n",
    "frequent_itemsets_baseline = model_fp.freqItemsets\n",
    "frequent_itemsets_baseline.cache()\n",
    "num_frequent_itemsets_baseline = frequent_itemsets_baseline.count()\n",
    "time_fp_itemsets = time.time() - start_time_fp\n",
    "logger.info(f\"FPGrowth Baseline: Found {num_frequent_itemsets_baseline} frequent itemsets in {time_fp_itemsets:.2f} seconds.\")\n",
    "logger.info(\"Sample of FPGrowth Baseline Frequent Itemsets:\")\n",
    "frequent_itemsets_baseline.show(10, truncate=False)\n",
    "\n",
    "\n",
    "# --- 2. Generate Association Rules (Baseline) ---\n",
    "logger.info(\"Generating Association Rules (Baseline) from Frequent Itemsets...\")\n",
    "start_time_rules = time.time()\n",
    "\n",
    "# Use AssociationRules transformer to generate rules from frequent itemsets\n",
    "# minConfidence is applied here\n",
    "association_rules_generator = AssociationRules(minConfidence=MIN_CONFIDENCE)\n",
    "association_rules_baseline = association_rules_generator.transform(frequent_itemsets_baseline)\n",
    "association_rules_baseline.cache()\n",
    "\n",
    "num_association_rules_baseline = association_rules_baseline.count()\n",
    "time_rules_baseline = time.time() - start_time_rules\n",
    "logger.info(f\"Association Rules Baseline: Found {num_association_rules_baseline} rules in {time_rules_baseline:.2f} seconds.\")\n",
    "logger.info(\"Sample of FPGrowth Baseline Association Rules (by confidence):\")\n",
    "association_rules_baseline.orderBy(F.desc(\"confidence\")).show(10, truncate=False)\n",
    "\n",
    "# --- Clean up baseline caches ---\n",
    "transactions_df.unpersist()\n",
    "frequent_itemsets_baseline.unpersist()\n",
    "association_rules_baseline.unpersist()\n",
    "\n",
    "# --- End of Baseline Script ---\n",
    "logger.info(\"FPGrowth Baseline comparison script finished.\")\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
